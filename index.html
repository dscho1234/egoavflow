<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="EgoAVFlow: Robot Policy Learning with Active Vision from Human Egocentric Videos via 3D Flow">
  <meta name="keywords" content="EgoAVFlow, Robot Policy, Active Vision, Egocentric Videos, 3D Flow">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>EgoAVFlow: Robot Policy Learning with Active Vision from Human Egocentric Videos via 3D Flow</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$']],
        displayMath: [['$$', '$$']],
        packages: {'[+]': ['color']}
      }
    };
  </script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" async></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <!--
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://keunhong.com">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://hypernerf.github.io">
            HyperNeRF
          </a>
          <a class="navbar-item" href="https://nerfies.github.io">
            Nerfies
          </a>
          <a class="navbar-item" href="https://latentfusion.github.io">
            LatentFusion
          </a>
          <a class="navbar-item" href="https://photoshape.github.io">
            PhotoShape
          </a>
        </div>
      </div>
    </div>

  </div>
  -->
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">EgoAVFlow: Robot Policy Learning with Active Vision from Human Egocentric Videos via 3D Flow</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              Anonymous Author
            </span>
          </div>

          <!--
          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://github.com/google/nerfies"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
            </div>

          </div>
          -->
        </div>
      </div>
    </div>
  </div>
</section>

<!--
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/teaser.mp4"
                type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">Nerfies</span> turns selfie videos from your phone into
        free-viewpoint
        portraits.
      </h2>
    </div>
  </div>
</section>
-->


<!--
<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
          <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/steve.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-chair-tp">
          <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/chair-tp.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/shiba.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-fullbody">
          <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/fullbody.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-blueshirt">
          <video poster="" id="blueshirt" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/blueshirt.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-mask">
          <video poster="" id="mask" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/mask.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-coffee">
          <video poster="" id="coffee" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/coffee.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-toby">
          <video poster="" id="toby" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/toby2.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>
-->


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Learning robot manipulation from egocentric human videos is attractive for scaling beyond robot data collection, 
            yet direct transfer is brittle because successful execution requires active vision---maintaining informative viewpoints to avoid perceptual failures during manipulation. 
            We propose <strong>EgoAVFlow</strong>, a human-egocentric-video-only framework that jointly learns manipulation and viewpoint adjustment using a shared 3D flow representation that is viewpoint-robust and bridges the human--robot embodiment gap. 
            EgoAVFlow trains a robot manipulation policy, a 3D flow prediction model, and a view policy that maximizes object visibility via a visibility-aware reward and test-time reward-maximizing diffusion denoising. 
            Real-world experiments under actively changing viewpoints show that EgoAVFlow consistently outperforms prior human-demo-based baselines, demonstrating effective visibility maintenance and robust manipulation without robot demonstrations.
            
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <video id="paper-video" autoplay controls muted loop playsinline width="100%">
            <source src="./static/videos/videos-v3-wo-appendix-compressed.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
    <!--/ Paper video. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Method. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Method</h2>
        
        <!-- Thumbnail Image -->
        <div class="columns is-centered">
          <div class="column is-four-fifths">
            <div class="content">
              <img src="./static/images/thumbnail-v3.png"
                   alt="Thumbnail image"
                   style="width: 100%;"/>
              <p class="has-text-justified">
                Given egocentric human videos with varying visibility, EgoAVFlow converts the demonstrations into a 3D flow representation. Building on this representation, it jointly learns a robot manipulation policy and a view policy that adjusts the camera viewpoint independently of the human demo to maximize the visibility of the object of interest.
              </p>
            </div>
          </div>
        </div>

        <!-- Method Overview Image -->
        <div class="columns is-centered">
          <div class="column is-four-fifths">
            <div class="content">
              <img src="./static/images/method-v3.png"
                   alt="Method overview diagram"
                   style="width: 100%;"/>
              <p class="has-text-justified">
                <strong>Method Overview:</strong> EgoAVFlow consists of three diffusion models. The robot policy $\pi_r$ produces future robot action sequences. The flow generation model $f$ predicts future 3D flows given the $\pi_r$'s outputs. The view policy $\pi_v$ produces future camera viewpoints given the $\pi_r$, $f$'s outputs, and reconstructed mesh surfaces through a visibility-aware reward-maximizing denoising process. <span style="color: violet; font-weight: bold;">Viewpoints (A)</span> represents that most query points are invisible (<span style="color: red; font-weight: bold;">Red LOS</span>) due to the table's mesh surface or out of FoV, while these points are visible (<span style="color: #22c55e; font-weight: bold;">Green LOS</span>) in <span style="color: cyan; font-weight: bold;">viewpoints (B)</span>.
              </p>
            </div>
          </div>
        </div>

      </div>
    </div>
    <!--/ Method. -->
  </div>
</section>


<!-- Dataset -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Dataset</h2>

        <div class="columns">
          <!-- Good visibility -->
          <div class="column is-half">
            <h3 class="title is-4">Good visibility</h3>
            <div class="columns">
              <div class="column is-half">
                <video autoplay controls muted loop playsinline width="100%">
                  <source src="./static/videos/spray_good.mp4" type="video/mp4">
                </video>
              </div>
              <div class="column is-half">
                <video autoplay controls muted loop playsinline width="100%">
                  <source src="./static/videos/doll_good.mp4" type="video/mp4">
                </video>
              </div>
            </div>
            <div class="columns">
              <div class="column is-half">
                <video autoplay controls muted loop playsinline width="100%">
                  <source src="./static/videos/towel_good.mp4" type="video/mp4">
                </video>
              </div>
              <div class="column is-half">
                <video autoplay controls muted loop playsinline width="100%">
                  <source src="./static/videos/toilet_paper_good.mp4" type="video/mp4">
                </video>
              </div>
            </div>
          </div>
          <!-- Bad visibility -->
          <div class="column is-half">
            <h3 class="title is-4">Bad visibility</h3>
            <div class="columns">
              <div class="column is-half">
                <video autoplay controls muted loop playsinline width="100%">
                  <source src="./static/videos/spray_bad.mp4" type="video/mp4">
                </video>
              </div>
              <div class="column is-half">
                <video autoplay controls muted loop playsinline width="100%">
                  <source src="./static/videos/doll_bad.mp4" type="video/mp4">
                </video>
              </div>
            </div>
            <div class="columns">
              <div class="column is-half">
                <video autoplay controls muted loop playsinline width="100%">
                  <source src="./static/videos/towel_bad.mp4" type="video/mp4">
                </video>
              </div>
              <div class="column is-half">
                <video autoplay controls muted loop playsinline width="100%">
                  <source src="./static/videos/toilet_paper_bad.mp4" type="video/mp4">
                </video>
              </div>
            </div>
          </div>
        </div>

        <div class="content has-text-justified" style="margin-top: 1.5rem;">
          <p>
            <strong>Task description:</strong> [There are 4 different tasks: spray, doll, towel, toilet paper. Each task requires appropriate viewpoint adjustments. Otherwise, the object is occluded by the robot or elements in the environment, such as a table or drawer.]
          </p>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Q1: Is the viewpoint adjustment necessary? -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Q1: Is the viewpoint adjustment necessary?</h2>
        <div class="columns is-centered">
          <div class="column is-four-fifths">
            <div class="content">
              <img src="./static/images/visibility_comparison-v2.png"
                   alt="Visibility comparison"
                   style="width: 100%;"/>
              <p class="has-text-justified">
                The visibility is computed from each different fixed viewpoint. No single viewpoint can maintain full visibility throughout the execution, indicating that the viewpoint must be continuously adjusted online to maximize visibility.
              </p>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Q2: Is imitating a human's viewpoints the proper way? -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Q2: Is imitating a human's viewpoints the proper way?</h2>

        <div class="columns">
          <!-- Ours -->
          <div class="column is-half">
            <h3 class="title is-4">Ours</h3>
            <div class="columns">
              <div class="column is-half">
                <video autoplay controls muted loop playsinline width="100%">
                  <source src="./static/videos/spray_good_robot_trim.mp4" type="video/mp4">
                </video>
              </div>
              <div class="column is-half">
                <video autoplay controls muted loop playsinline width="100%">
                  <source src="./static/videos/doll_good_robot_trim.mp4" type="video/mp4">
                </video>
              </div>
            </div>
            <div class="columns">
              <div class="column is-half">
                <video autoplay controls muted loop playsinline width="100%">
                  <source src="./static/videos/towel_good_robot_trim.mp4" type="video/mp4">
                </video>
              </div>
              <div class="column is-half">
                <video autoplay controls muted loop playsinline width="100%">
                  <source src="./static/videos/toilet_paper_good_robot_trim.mp4" type="video/mp4">
                </video>
              </div>
            </div>
          </div>
          <!-- Naive AV -->
          <div class="column is-half">
            <h3 class="title is-4">Naive AV</h3>
            <div class="columns">
              <div class="column is-half">
                <video autoplay controls muted loop playsinline width="100%">
                  <source src="./static/videos/spray_bad_robot_trim.mp4" type="video/mp4">
                </video>
              </div>
              <div class="column is-half">
                <video autoplay controls muted loop playsinline width="100%">
                  <source src="./static/videos/doll_bad_robot_trim.mp4" type="video/mp4">
                </video>
              </div>
            </div>
            <div class="columns">
              <div class="column is-half">
                <video autoplay controls muted loop playsinline width="100%">
                  <source src="./static/videos/towel_bad_robot_trim.mp4" type="video/mp4">
                </video>
              </div>
              <div class="column is-half">
                <video autoplay controls muted loop playsinline width="100%">
                  <source src="./static/videos/toilet_paper_bad_robot_trim.mp4" type="video/mp4">
                </video>
              </div>
            </div>
          </div>
        </div>

        <div class="content has-text-justified" style="margin-top: 1.5rem;">
          <p>
            Due to the visibility-maximizing viewpoint adjustments, EgoAVFlow maintains visibility of the query points and their predicted future flows, whereas Naive AV fails to keep them in view, causing the query points to move out of the FoV.
          </p>
        </div>

        <div class="columns is-centered" style="margin-top: 1.5rem;">
          <div class="column is-half">
            <div class="content">
              <img src="./static/images/success_rate_av.png" alt="Success rate AV" style="width: 100%;"/>
              <p class="has-text-justified">
                <strong>Success rate (AV):</strong> Success rates of EgoAVFlow and Naive AV. 
              </p>
            </div>
          </div>
          <div class="column is-half">
            <div class="content">
              <img src="./static/images/visibility_reward.png" alt="Visibility reward" style="width: 100%;"/>
              <p class="has-text-justified">
                <strong>Visibility reward:</strong> For all tasks, EgoAVFlow achieves higher average visibility rewards $R_{vis}$ than Naive AV, demonstrating our method's visibility maintenance capability.
              </p>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Q3: Is the proposed 3D flow-based policy robust under actively varying viewpoints? -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Q3: Is the proposed 3D flow-based policy robust under actively varying viewpoints?</h2>

        <div class="columns">
          <!-- Left: 2x2 videos with method labels -->
          <div class="column is-half">
            <div class="columns">
              <div class="column is-half">
                <video autoplay controls muted loop playsinline width="100%">
                  <source src="./static/videos/spray_good_robot_trim.mp4" type="video/mp4">
                </video>
                <p class="has-text-centered" style="margin-top: 0.35rem; font-size: 0.95rem;"><strong>EgoAVFlow (Ours)</strong></p>
              </div>
              <div class="column is-half">
                <video autoplay controls muted loop playsinline width="100%">
                  <source src="./static/videos/amplify_failure_trim.mp4" type="video/mp4">
                </video>
                <p class="has-text-centered" style="margin-top: 0.35rem; font-size: 0.95rem;"><strong>AMPLIFY</strong></p>
              </div>
            </div>
            <div class="columns">
              <div class="column is-half">
                <video autoplay controls muted loop playsinline width="100%">
                  <source src="./static/videos/egozero_failure_trim.mp4" type="video/mp4">
                </video>
                <p class="has-text-centered" style="margin-top: 0.35rem; font-size: 0.95rem;"><strong>EgoZero</strong></p>
              </div>
              <div class="column is-half">
                <video autoplay controls muted loop playsinline width="100%">
                  <source src="./static/videos/phantom_failure_trim.mp4" type="video/mp4">
                </video>
                <p class="has-text-centered" style="margin-top: 0.35rem; font-size: 0.95rem;"><strong>Phantom</strong></p>
              </div>
            </div>
          </div>
          <!-- Right: success rate figure -->
          <div class="column is-half">
            <div class="content">
              <img src="./static/images/success_rate_robot_policy_baseline.png" alt="Success rate robot policy baseline" style="width: 100%;"/>
              <p class="has-text-justified" style="margin-top: 1rem;">
                <strong>Success rate (robot policy baselines):</strong> Success rates of EgoAVFlow and robot policy learning baselines
              </p>
            </div>
          </div>
        </div>

        <div class="content has-text-justified" style="margin-top: 1.5rem;">
          <p>
            All baselines are trained on the same dataset as our method, and no real robot data is used. As AMPLIFY and Phantom require an image input, we use diffusion-based inpainting to remove the human in the image and overlays a robot onto the resulting frames, enabling policy training on synthesized robot observations. To isolate the effect of the policy representation, we use the same viewpoint adjustment module for all methods: our view policy $\pi_v$ with visibility-maximizing denoising. As shown in the experimental results, EgoAVFlow shows superior capability compared to baselines. As the viewpoint keeps changing during the evaluation by our proposed view policy, the results demonstrate that the proposed 3D flow-based policy is view-invariant and benefits from its inherent 3D representation.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Failure Analysis -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Failure Analysis</h2>
        <div class="columns is-centered">
          <div class="column is-four-fifths">
            <div class="content">
              <img src="./static/images/failure_analysis_pi_chart_only.png" alt="Failure analysis" style="width: 100%;"/>
              <p class="has-text-justified" style="margin-top: 1rem;">
                <strong>Failure analysis:</strong> Since AMPLIFY and Phantom are not inherently 3D-aware, they suffer from distribution shifts when evaluated under the unseen viewpoints. EgoZero uses 3D points, but it cannot address the non-static scene, such as when the object moves due to the gripper's contact. As a result, these robot policy baselines account for most manipulation-related failures, such as grasping/object pose failure. In the case of out-of-viewpoint, Naive AV accounts for most failures because it does not use our proposed view policy for visibility maintenance. EgoAVFlow accounts for the second-largest share of the failures. However, this does not indicate worse performance; rather, many robot policy baseline rollouts are already counted as early grasping failures and thus do not reach the later stages. If they progressed further, their proportions would be more comparable to our method.
              </p>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">

    <!-- Visual Effects & Matting
    <div class="columns is-centered">
      <div class="column">
        <div class="content">
          <h2 class="title is-3">Visual Effects</h2>
          <p>
            Using <i>nerfies</i> you can create fun visual effects. This Dolly zoom effect
            would be impossible without nerfies since it would require going through a wall.
          </p>
          <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/dollyzoom-stacked.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
      <div class="column">
        <h2 class="title is-3">Matting</h2>
        <div class="columns is-centered">
          <div class="column content">
            <p>
              As a byproduct of our method, we can also solve the matting problem by ignoring
              samples that fall outside of a bounding box during rendering.
            </p>
            <video id="matting-video" controls playsinline height="100%">
              <source src="./static/videos/matting.mp4"
                      type="video/mp4">
            </video>
          </div>
        </div>
      </div>
    </div>
    -->
    <!--/ Matting. -->

    <!--
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Animation</h2>

        <h3 class="title is-4">Interpolating states</h3>
        <div class="content has-text-justified">
          <p>
            We can also animate the scene by interpolating the deformation latent codes of two input
            frames. Use the slider here to linearly interpolate between the left frame and the right
            frame.
          </p>
        </div>
        <div class="columns is-vcentered interpolation-panel">
          <div class="column is-3 has-text-centered">
            <img src="./static/images/interpolate_start.jpg"
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>
            <p>Start Frame</p>
          </div>
          <div class="column interpolation-video-column">
            <div id="interpolation-image-wrapper">
              Loading...
            </div>
            <input class="slider is-fullwidth is-large is-info"
                   id="interpolation-slider"
                   step="1" min="0" max="100" value="0" type="range">
          </div>
          <div class="column is-3 has-text-centered">
            <img src="./static/images/interpolate_end.jpg"
                 class="interpolation-image"
                 alt="Interpolation end reference image."/>
            <p class="is-bold">End Frame</p>
          </div>
        </div>
        <br/>

        <h3 class="title is-4">Re-rendering the input video</h3>
        <div class="content has-text-justified">
          <p>
            Using <span class="dnerf">Nerfies</span>, you can re-render a video from a novel
            viewpoint such as a stabilized camera by playing back the training deformations.
          </p>
        </div>
        <div class="content has-text-centered">
          <video id="replay-video"
                 controls
                 muted
                 preload
                 playsinline
                 width="75%">
            <source src="./static/videos/replay.mp4"
                    type="video/mp4">
          </video>
        </div>

      </div>
    </div>


    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Related Links</h2>

        <div class="content has-text-justified">
          <p>
            There's a lot of excellent work that was introduced around the same time as ours.
          </p>
          <p>
            <a href="https://arxiv.org/abs/2104.09125">Progressive Encoding for Neural Optimization</a> introduces an idea similar to our windowed position encoding for coarse-to-fine optimization.
          </p>
          <p>
            <a href="https://www.albertpumarola.com/research/D-NeRF/index.html">D-NeRF</a> and <a href="https://gvv.mpi-inf.mpg.de/projects/nonrigid_nerf/">NR-NeRF</a>
            both use deformation fields to model non-rigid scenes.
          </p>
          <p>
            Some works model videos with a NeRF by directly modulating the density, such as <a href="https://video-nerf.github.io/">Video-NeRF</a>, <a href="https://www.cs.cornell.edu/~zl548/NSFF/">NSFF</a>, and <a href="https://neural-3d-video.github.io/">DyNeRF</a>
          </p>
          <p>
            There are probably many more by the time you are reading this. Check out <a href="https://dellaert.github.io/NeRF/">Frank Dellart's survey on recent NeRF papers</a>, and <a href="https://github.com/yenchenlin/awesome-NeRF">Yen-Chen Lin's curated list of NeRF papers</a>.
          </p>
        </div>
      </div>
    </div>
    -->

  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content has-text-centered">
          <p>
            This website template is borrowed from <a href="https://nerfies.github.io">Nerfies project webpage</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

<!--
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{park2021nerfies,
  author    = {Park, Keunhong and Sinha, Utkarsh and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Seitz, Steven M. and Martin-Brualla, Ricardo},
  title     = {Nerfies: Deformable Neural Radiance Fields},
  journal   = {ICCV},
  year      = {2021},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>
-->

</body>
</html>
